{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruslanmv/Neural-Networks-from-Scratch/blob/master/3_Visualizing_Intermediate_Representations_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-74XLLwqPlcw"
      },
      "source": [
        "# Visualizing-Intermediate-Representations\n",
        "\n",
        "In this blog post, we will build a train a model on single class of the MMORPG-AI dataset. We want to predict the RX motion of the gamepad. This contains images of Gennshin Impact Gameplay. You will use the [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) class to prepare this dataset so it can be fed to a convolutional neural network.\n",
        "\n",
        "**IMPORTANT NOTE:** This notebook is designed to run as a Colab. Running it on your local machine might result in some of the code blocks throwing errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYFguQkJvpV3"
      },
      "source": [
        "Run the code below to download the compressed dataset `classes.zip`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXZT2UsyIVe_"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/ruslanmv/BOT-MMORPG-AI/master/versions/0.01/clean_reduced/classes.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9brUxyTpYZHy"
      },
      "source": [
        "You can then unzip the archive using the [zipfile](https://docs.python.org/3/library/zipfile.html) module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLy3pthUS0D2"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip the dataset\n",
        "local_zip = './classes.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-qUPyfO7Qr8"
      },
      "source": [
        "The contents of the .zip are extracted to the base directory `./classes`, which in turn each contain `rx_left` ,`rx_zero`, and `rx_right` subdirectories.\n",
        "\n",
        "In short: The training set is the data that is used to tell the neural network model that 'this is what a rx_left looks like' and 'this is what a rx_rightght looks like'.\n",
        "\n",
        "One thing to pay attention to in this sample: We do not explicitly label the images as rx_left or  rx_right. You will use the ImageDataGenerator API instead -- and this is coded to automatically label images according to the directory names and structure. \n",
        "\n",
        "So, for example, you will have a 'training' directory containing a ' rx_left ' directory and a 'rx_right ' one. `ImageDataGenerator` will label the images appropriately for you, reducing a coding step. \n",
        "\n",
        "You can now define each of these directories:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "N5_wswo6mEKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rm -r classes*"
      ],
      "metadata": {
        "id": "Q6anvzSzteD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls classes/\n"
      ],
      "metadata": {
        "id": "EAj5fRw6tMPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR_M9nWN-K8B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Directory with our training rx_left pictures\n",
        "train_rx_left_dir = os.path.join('./classes/rx_left/')\n",
        "\n",
        "# Directory with our training rx_right pictures\n",
        "train_rx_right_dir = os.path.join('./classes/rx_right/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_rx_left_dir"
      ],
      "metadata": {
        "id": "t6oCNx8iuQyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls ./classes/rx_left/"
      ],
      "metadata": {
        "id": "OMb9yqWbss9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuBYtA_Zd8_T"
      },
      "source": [
        "Now see what the filenames look like in the `rx_lefts` and `rx_rights` training directories:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_rx_left_names = os.listdir(train_rx_left_dir)\n",
        "print(train_rx_left_names[:10])"
      ],
      "metadata": {
        "id": "ifi6oncmunQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PIP1rkmeAYS"
      },
      "outputs": [],
      "source": [
        "train_rx_right_names = os.listdir(train_rx_right_dir)\n",
        "print(train_rx_right_names[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlqN5KbafhLI"
      },
      "source": [
        "You can also find out the total number of rx_left and rx_right images in the directories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4XHh2xSfgie"
      },
      "outputs": [],
      "source": [
        "print('total training rx_left images:', len(os.listdir(train_rx_left_dir)))\n",
        "print('total training rx_right images:', len(os.listdir(train_rx_right_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WZABE9eX-8"
      },
      "source": [
        "Now take a look at a few pictures to get a better sense of what they look like. First, configure the `matplotlib` parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2_Q0-_5UAv-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvHzGCxXkqp"
      },
      "source": [
        "Now, display a batch of 8 rx_left and 8 rx_right pictures. You can rerun the cell to see a fresh batch each time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpr8GxjOU8in"
      },
      "outputs": [],
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_rx_left_pix = [os.path.join(train_rx_left_dir, fname) \n",
        "                for fname in train_rx_left_names[pic_index-8:pic_index]]\n",
        "next_rx_right_pix = [os.path.join(train_rx_right_dir, fname) \n",
        "                for fname in train_rx_right_names[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_rx_left_pix+next_rx_right_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqBkNBJmtUv"
      },
      "source": [
        "## Building a Small Model from Scratch\n",
        "\n",
        "Now you can define the model architecture that you will train.\n",
        "\n",
        "Step 1 will be to import tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We define the parameters\n",
        "width= 480\n",
        "height= 270\n",
        "ncolors=3"
      ],
      "metadata": {
        "id": "JZR8dLI-3Cvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvfZg3LQbD-5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnhYCP4tdqjC"
      },
      "source": [
        "You then add convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers. \n",
        "\n",
        "\n",
        "Note that if there is a two-class classification problem, i.e. a *binary classification problem*, you will end your network with a [*sigmoid* activation](https://wikipedia.org/wiki/Sigmoid_function). \n",
        "\n",
        "This makes the output value of your network a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0).\n",
        "\n",
        "However for multi-class classification problem, i.e. a *non binary classification problem*, you will end your network with a [*softmax* activation](https://wikipedia.org/wiki/Softmax_function). \n",
        "\n",
        "The softmax function, also known as softargmax or normalized exponential function, is a generalization of the logistic function to multiple dimensions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PixZ2s5QbYQ3"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=( width,height, ncolors)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "\n",
        "    # For binary problem\n",
        "    \n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('rx_lefts') and 1 for the other ('rx_rights')\n",
        "    #tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    # For non binary problem\n",
        "    \n",
        "    # Only 3 output neuron for 1 class ('rx_right') the value of 1 \n",
        "    #                     for 2 class ('rx_lefts') the value of -1\n",
        "    #                     for 3 class ('rx_zero') the value of zero\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9EaFDP5srBa"
      },
      "source": [
        "You can review the network architecture and the output shapes with `model.summary()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZKj8392nbgP"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmtkTn06pKxF"
      },
      "source": [
        "The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers removes the outermost pixels of the image, and each pooling layer halves the dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEkKSpZlvJXA"
      },
      "source": [
        "Next, you'll configure the specifications for model training. \n",
        "\n",
        "If we had the vinnary problem the model should have [`binary_crossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) loss because it's a binary classification problem, and the final activation is a sigmoid. (For a refresher on loss metrics, see this [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/descending-into-ml/video-lecture).) \n",
        "\n",
        "\n",
        "\n",
        "However for the multiclass we need \n",
        "\n",
        "\n",
        "[`categorical_crossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) \n",
        "\n",
        "\n",
        "We use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature.\n",
        "\n",
        "We can also use  the `rmsprop` optimizer with a learning rate of `0.001`. During training, you will want to monitor classification accuracy.\n",
        "\n",
        "**NOTE**: In this case, using the [RMSprop optimization algorithm](https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) is preferable to [stochastic gradient descent](https://developers.google.com/machine-learning/glossary/#SGD) (SGD), because RMSprop automates learning-rate tuning for us. (Other optimizers, such as [Adam](https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam) and [Adagrad](https://developers.google.com/machine-learning/glossary/#AdaGrad), also automatically adapt the learning rate during training, and would work equally well here.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DHWhFP_uhq3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# for binary\n",
        "#model.compile(loss='binary_crossentropy',\n",
        "#              optimizer=RMSprop(learning_rate=0.001),\n",
        "#              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# for no binary\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn9m9D3UimHM"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Next step is to set up the data generators that will read pictures in the source folders, convert them to `float32` tensors, and feed them (with their labels) to the model. You'll have one generator for the training images and one for the validation images. These generators will yield batches of images of size 480x270 and their labels (binary).\n",
        "\n",
        "As you may already know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network (i.e. It is uncommon to feed raw pixels into a ConvNet.) In this case, you will preprocess the images by normalizing the pixel values to be in the `[0, 1]` range (originally all values are in the `[0, 255]` range).\n",
        "\n",
        "In Keras, this can be done via the `keras.preprocessing.image.ImageDataGenerator` class using the `rescale` parameter. This `ImageDataGenerator` class allows you to instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "class_mode: One of \"categorical\", \"binary\", \"sparse\", \"input\", or None. \n",
        "\n",
        "Default: \"categorical\". Determines the type of label arrays that are returned: \n",
        "\n",
        "- \"categorical\" will be 2D one-hot encoded labels,\n",
        "- \"binary\" will be 1D binary labels, \n",
        "- \"sparse\" will be 1D integer labels, \n",
        "- \"input\" will be images identical to input images (mainly used to work with autoencoders). \n",
        "- If None, no labels are returned (the generator will only yield batches of image data, which is useful to use with model.predict_generator()). \n",
        "\n",
        "Please note that in case of class_mode None, the data still needs to reside in a subdirectory of directory for it to work correctly.\n",
        "\n",
        "\n",
        "Categorical_crossentropy ( cce ) produces a one-hot array containing the probable match for each category.\n",
        "\n",
        "Sparse_categorical_crossentropy ( scce ) produces a category index of the most likely matching category.\n",
        "\n",
        "**Summarizing:**\n",
        "### Class modes:\n",
        "- \"categorical\": 2D output (aka. list of numbers of length N), [0, 0, 1, 0], which is a one-hot encoding (only one number is 1/ \"hot\") representing the donkey. This is for mutually exclusive labels. A dog cannot be a cat, a human is not a dog.\n",
        "\n",
        "- \"binary\": 1D output (aka. 1 number), which is either 0, 1, 2, 3 ... N. It is called this because it is binary if there are only two classes (IMHO this is a bad reason), source. I suggest using \"binary\" just for single label classification, because it documents-in-code, your intention.\n",
        "\n",
        "- \"sparse\": After digging in the code, this is the same as \"binary\". The logic is done with elif self.class_mode in {'binary', 'sparse'}:, and the class_mode is not used after that. I suggest using \"sparse\" for multilabel classification though, again because it documents-in-code, your intention.\n",
        "\n",
        "- \"input\": The label is literally the image again. So the label for an image of the dog, is the same dog picture array. If I knew more about autoencoders I might have been able to explain further.\n",
        "\n",
        "- None: No labels, therefore not useful for training, but for inference/ prediction"
      ],
      "metadata": {
        "id": "M2Hx4aND4qzq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClebU9NJg99G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        './classes/',  # This is the source directory for training images\n",
        "        target_size=(480, 270),  # All images will be resized to 480x270\n",
        "        batch_size=128,\n",
        "\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        #class_mode='binary'\n",
        "        \n",
        "        # Since we use categorical_crossentropy\n",
        "        class_mode='categorical'\n",
        "        \n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu3Jdwkjwax4"
      },
      "source": [
        "### Training\n",
        "\n",
        "You can start training for 50 epochs -- this may take a few minutes to run.\n",
        "\n",
        "Do note the values per epoch.\n",
        "\n",
        "The `loss` and `accuracy` are great indicators of progress in training. `loss` measures the current model prediction against the known labels, calculating the result. `accuracy`, on the other hand, is the portion of correct guesses. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb1_lgobv81m"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=2,  \n",
        "      epochs=50,\n",
        "      verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6vSHzPR2ghH"
      },
      "source": [
        "### Model Prediction\n",
        "\n",
        "Now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, upload them, and run them through the model, giving an indication of whether the object is a rx_left or a rx_right.\n",
        "\n",
        "**Important Note:** Due to some compatibility issues, the following code block will result in an error after you select the images(s) to upload if you are running this notebook as a `Colab` on the `Safari` browser. For all other browsers, continue with the next code block and ignore the next one after it.\n",
        "Upload widget is only available when the cell has been executed in the current browser session.\n",
        "Does not work in Incognito Google Browser\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E3k3qBMNlWHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoWp43WxJDNT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(480, 270))\n",
        "  x = image.img_to_array(img)\n",
        "  x /= 255\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "\n",
        "  number_list=list(classes[0])\n",
        "  max_value = max(number_list) \n",
        "  max_index = number_list.index(max_value)\n",
        "  print(max_index)\n",
        "\n",
        "  if max_index == 0 :\n",
        "     print(\"Belong to First Class - rx left\")   \n",
        "\n",
        "  if max_index == 1 :\n",
        "     print(\"Belong to Second Class - rx right\")\n",
        "\n",
        "  if max_index == 2 :\n",
        "     print(\"Belong to Third Class - rx zero\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8EHQyWGDvWz"
      },
      "source": [
        "### Visualizing Intermediate Representations\n",
        "\n",
        "To get a feel for what kind of features your CNN has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the model.\n",
        "\n",
        "You can pick a random image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)"
      ],
      "metadata": {
        "id": "fB2I6ij-PRnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a random input image from the training set.\n",
        "rx_left_img_files = [os.path.join(train_rx_left_dir, f) for f in train_rx_left_names]"
      ],
      "metadata": {
        "id": "l31OhH7gPaf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rx_right_img_files = [os.path.join(train_rx_right_dir, f) for f in train_rx_right_names]\n",
        "img_path = random.choice(rx_left_img_files + rx_right_img_files)"
      ],
      "metadata": {
        "id": "lni38ZvYPdWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path"
      ],
      "metadata": {
        "id": "d6nx771oQMpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "width,height, ncolors"
      ],
      "metadata": {
        "id": "7aR4phQuPzdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  img_test = mpimg.imread(img_path)\n",
        "  plt.imshow(img_test)"
      ],
      "metadata": {
        "id": "9fZPxYr5knLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = load_img(img_path, target_size=(width,height))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (width,height, ncolors)"
      ],
      "metadata": {
        "id": "QXFl-mBSP80e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, width,height, 3)\n",
        "\n",
        "# Scale by 1/255\n",
        "x /= 255"
      ],
      "metadata": {
        "id": "OS1r93JnQBaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the image through the network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)"
      ],
      "metadata": {
        "id": "gjVOniJtQT6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the names of the layers, so you can have them as part of the plot\n",
        "layer_names = [layer.name for layer in model.layers[1:]]"
      ],
      "metadata": {
        "id": "uM7xhPZZQXfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "\n",
        "    # The feature map has shape (1, size1, size2, n_features)\n",
        "    size1 = feature_map.shape[1]\n",
        "    size2 = feature_map.shape[2]\n",
        "    print(size1,size2)"
      ],
      "metadata": {
        "id": "GUzFRSE3Q_2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    #size = feature_map.shape[1]\n",
        "    \n",
        "    # The feature map has shape (1, size1, size2, n_features)\n",
        "    size1 = feature_map.shape[1]\n",
        "    size2 = feature_map.shape[2]\n",
        "\n",
        "    # Tile the images in this matrix\n",
        "    display_grid = np.zeros((size1, size2 * n_features))\n",
        "    for i in range(n_features):\n",
        "      x = feature_map[0, :, :, i]\n",
        "     # print(x)\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "\n",
        "      inf=i*size1\n",
        "      sup=(i*size1 + size2)\n",
        "\n",
        "      inf_trucco=x.shape[0]\n",
        "      sup_trucco=x.shape[1]\n",
        "      check=display_grid[:,inf:sup].shape\n",
        "      check_y=check[1]\n",
        "    \n",
        "      # Tile each filter into this big horizontal grid\n",
        "      #display_grid[:, i * size : (i + 1) * size] = x\n",
        "      if sup_trucco == check_y :\n",
        "          display_grid[:, inf : sup] = x\n",
        "\n",
        "    \n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "metadata": {
        "id": "9ph17SZBR4g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuqK2arJL0wo"
      },
      "source": [
        "You can see above how the pixels highlighted turn to increasingly abstract and compact representations, especially at the bottom grid. \n",
        "\n",
        "The representations downstream start highlighting what the network pays attention to, and they show fewer and fewer features being \"activated\"; most are set to zero. This is called _representation sparsity_ and is a key feature of deep learning. These representations carry increasingly less information about the original pixels of the image, but increasingly refined information about the class of the image. You can think of a convnet (or a deep network in general) as an information distillation pipeline wherein each layer filters out the most useful features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IBgYCYooGD"
      },
      "source": [
        "## Congratulations\n",
        "\n",
        "We have learned how to visualize, train and create a Neural Network model with MMORPG-AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "651IgjLyo-Jx"
      },
      "outputs": [],
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "3-Visualizing-Intermediate-Representations",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}