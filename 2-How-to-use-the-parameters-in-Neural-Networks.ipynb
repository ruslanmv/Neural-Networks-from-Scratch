{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7a5211",
   "metadata": {},
   "source": [
    "# How to use the parameters in Neural-Networks\n",
    "\n",
    "\n",
    "Today I will discuss about how to adjust the parameters of the neural networks from unknonwn Neural Networks.\n",
    "\n",
    "\n",
    "\n",
    "In ordering to select the appropiate parameters in a simple Neural Network for example in case of a Convolutional  Neural Network , we should remember the meaning of all the layers and understand the \n",
    "the following parameters:\n",
    "\n",
    "- Activation Shape\n",
    "- Activation Size \n",
    "- Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76edfc4",
   "metadata": {},
   "source": [
    "# Analysis of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f59fb6",
   "metadata": {},
   "source": [
    "To build the neural network, we should know the dimensions of the layers that are include in the network.\n",
    "\n",
    "In this work we will use three types of layers in a convolution\n",
    "- Convolution (CONV)\n",
    "- Pooling  (POOL) \n",
    "- Fully connected (FC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df4bda",
   "metadata": {},
   "source": [
    "### Parameters in Convolution Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc34b38",
   "metadata": {},
   "source": [
    "To understand better the layers we have to understand the parameters involved in the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8656100",
   "metadata": {},
   "source": [
    "Let us define several helper functions that allow us understand how the Neural Networks works and use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a26496a",
   "metadata": {},
   "source": [
    "## Convolution (CONV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f2cf0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_valid_convolution(inputs,  kernel):\n",
    "    '''\n",
    "    input\n",
    "    nh : height\n",
    "    nw : widht\n",
    "    \n",
    "    kernel\n",
    "    fh : filter\n",
    "    fw : filter\n",
    "    '''\n",
    "    nh,nw,= inputs\n",
    "    fh,fw = kernel\n",
    "    return (nh-fh) + 1, (nw-fw) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "14e2f00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = 6 , 6  # nxn image\n",
    "filters = 3 , 3 # fxf filter\n",
    "dim_valid_convolution( inputs, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1776b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_same_convolution(inputs,  kernel,s,p):\n",
    "    '''\n",
    "    Output size is the same as input size\n",
    "    \n",
    "    input\n",
    "    nh : height\n",
    "    nw : widht\n",
    "    \n",
    "    kernel\n",
    "    fh : filter\n",
    "    fw : filter\n",
    "    '''\n",
    "    nh,nw,= inputs\n",
    "    fh,fw = kernel\n",
    "    return (nh+2*p-fh) + 1, (nw+2*p-fw) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a76cae",
   "metadata": {},
   "source": [
    "We choose pad in a way that the output size is the same as the input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f72c1868",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 6 , 6  # nxn image\n",
    "filters = 3 , 3 # fxf filter\n",
    "stride=1.0    #stride s\n",
    "padding=1.0   # padding s\n",
    "parameters=dim_same_convolution( inputs, filters,stride,padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "124fc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_same(inputs,parameters):\n",
    "    #2D\n",
    "    if len(parameters)==2 :\n",
    "        assert parameters[0] == inputs[0] and  parameters[1] ==inputs[1],\"It is not same convolution, please fix the stride or padding for the input\"+str(inputs)+\"and parameters \"+str(parameters)      \n",
    "    #3D    \n",
    "    if len(parameters)==3 :\n",
    "        assert parameters[0][0] == inputs[0] and  parameters[0][1] ==inputs[1],\"It is not same convolution, please fix the stride or padding for the input\"+str(inputs)+\"and parameters \"+str(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e24796c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_same(inputs,parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a319d2a",
   "metadata": {},
   "source": [
    "![strided](img/strided.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c569553",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dim_strided_convolution(inputs, kernel ,s,p):\n",
    "    '''\n",
    "    input = (nh, nw)\n",
    "    nh : height\n",
    "    nw : widht\n",
    "    \n",
    "    kernel = (fh, fw)\n",
    "    fh : filter height\n",
    "    fw : filter widht\n",
    "\n",
    "    p : padding\n",
    "    s : stride\n",
    "\n",
    "    '''\n",
    "    nh,nw= inputs\n",
    "    fh,fw= kernel\n",
    "    \n",
    "    \n",
    "    print(\"Activation Shape Strided\")\n",
    "\n",
    "    \n",
    "    return (nh+2*p-fh)/s + 1, (nw+2*p-fw)/s + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d741a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape Strided\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.0, 3.0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = 7,7  # nxn image\n",
    "kernel = 3,3  # fxf filter\n",
    "stride=2.0    #stride s\n",
    "padding=0.0   # padding s\n",
    "dim_strided_convolution(inputs, kernel ,stride,padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6437462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_rgb_convolution(inputs, kernel,stride,padding,filters):\n",
    "    '''\n",
    "    input = (nh, nw, nc)\n",
    "    where \n",
    "    nh: height\n",
    "    nw: widht\n",
    "    nc: channels\n",
    "    \n",
    "    output = (nhl,nwl,ncl)\n",
    "       \n",
    "    nhl = (nh+2*p-fw)/s + 1\n",
    "    nwl = (nw+2*p-fh)/s + 1\n",
    "    ncl = filters\n",
    "    \n",
    "    \n",
    "    where\n",
    "       fw,fh : filter sizes\n",
    "       p : padding\n",
    "       s : stride  \n",
    "    ncl  : filters\n",
    "    \n",
    "    '''\n",
    "    nh,nw,nc = inputs\n",
    "    fh,fw = kernel\n",
    "     \n",
    "    s        = stride\n",
    "    p        = padding\n",
    "    ncl      = filters\n",
    "\n",
    "    nhl = (nh+2*p-fw)/s + 1\n",
    "    nwl = (nw+2*p-fh)/s + 1\n",
    "    output = (int(nhl),int(nwl),int(ncl))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Activation Shape\")\n",
    "\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55798a78",
   "metadata": {},
   "source": [
    "Let us define the number of parameters used in each convolution,\n",
    "The paremeters are defined as  ((shape of width of filter x shape of height filter x number of filters in the previous layer+1) x number of filters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d90c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nparameters_convolution(inputs, kernel,stride,padding,filters):\n",
    "    '''\n",
    "    input = (nh, nw, nc)\n",
    "    where \n",
    "    nh: height\n",
    "    nw: widht\n",
    "    nc: channels\n",
    "    \n",
    "    activation_shape  = (nhl,nwl,ncl)\n",
    "       \n",
    "    nhl = (nh+2*p-fw)/s + 1\n",
    "    nwl = (nw+2*p-fh)/s + 1\n",
    "    ncl = filters\n",
    "    \n",
    "    \n",
    "    where\n",
    "       fw,fh : filter sizes\n",
    "       p : padding\n",
    "       s : stride  \n",
    "    ncl  : filters\n",
    "    \n",
    "    '''\n",
    "    nh,nw,nc = inputs\n",
    "    fh,fw = kernel\n",
    "     \n",
    "    s        = stride\n",
    "    p        = padding\n",
    "    ncl      = filters\n",
    "    \n",
    "    #activation shape \n",
    "    nhl = (nh+2*p-fw)/s + 1\n",
    "    nwl = (nw+2*p-fh)/s + 1\n",
    "    activation_shape = (int(nhl),int(nwl),int(ncl))\n",
    "    \n",
    "    # activation size\n",
    "    activation_size=int(nhl)*int(nwl)*int(ncl)\n",
    "    \n",
    "    \n",
    "    # number Parameters\n",
    "    nparameters=((fh*fw*nc)+1)*ncl\n",
    "    \n",
    "    print(\"Activation Shape,\", \"Activation Size,\",\"# Parameters\")\n",
    "     \n",
    "    return   activation_shape ,  activation_size, nparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d389fc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((28, 28, 8), 6272, 608)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = 32,32,3  #nw x nh x nc image\n",
    "kernel  = 5,5      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "filters = 8       #number of filters ncl\n",
    "\n",
    "\n",
    "\n",
    "nparameters_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934cd63",
   "metadata": {},
   "source": [
    "![convnet](img/convnet.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16e61108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37, 37, 10)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = 39,39,3  #nw x nh x nc image\n",
    "kernel  = 3,3      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "filters = 10       #number of filters ncl\n",
    "\n",
    "dim_rgb_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a6b55",
   "metadata": {},
   "source": [
    "Where  the activation size, considering it’s merely the product of width, height and the number of channels in that layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323d7cb",
   "metadata": {},
   "source": [
    "The input layer’s shape is (37, 37, 10), the activation size of that layer is 37* 37* 10 = 13690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9fea408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13690"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "37* 37* 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7dc8cede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17, 17, 20)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = 37,37,10  #nw x nh x nc image\n",
    "kernel  = 5,5      #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "filters = 20       #number of filters ncl\n",
    "\n",
    "\n",
    "dim_rgb_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb88d8",
   "metadata": {},
   "source": [
    "The sme happens if we want to calculate the activation size for this convolution. All we have to do is just multiply (17, 17, 20) , i.e 17* 17* 20= 5780 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c86653be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5780"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17* 17* 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c1f5010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7, 7, 40)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = 17,17,20  #nw x nh x nc image\n",
    "kernel  = 5,5      #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "filters = 40       #number of filters ncl\n",
    "\n",
    "dim_rgb_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3ddea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((7, 7, 40), 1960, 20040)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nparameters_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6491364",
   "metadata": {},
   "source": [
    "The number of parameters in a given layer is the count of “learnable”  elements for a filter aka parameters for the filter for that layer. Parameters in general are weights that are learnt during training. They are weight matrices that contribute to model’s predictive power, changed during back-propagation process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70554f80",
   "metadata": {},
   "source": [
    "## Pooling (POOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5916a",
   "metadata": {},
   "source": [
    "In the pooling there are the following Hyperparameters:\n",
    "\n",
    "- f: filter size\n",
    "\n",
    "- s: stride\n",
    "- Max or average pooling\n",
    "\n",
    "Given an input with the dimensions\n",
    "\n",
    "$$n_H \\times n_W \\times n_C$$\n",
    "\n",
    "Max or  pooling is has the following dimensions\n",
    "\n",
    "$$ \\frac{n_H+2p-f}{s}+1 \\times \\frac{n_W+2p-f}{s}+1 \\times n_C$$ \n",
    "\n",
    "The numbers of channels remains $$n_C$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b11e2c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_pool(inputs, kernel,stride,padding):\n",
    "    '''\n",
    "    input = (nh, nw, nc)\n",
    "    where \n",
    "    nh: height\n",
    "    nw: widht\n",
    "    nc: channels\n",
    "    \n",
    "    activation_shape  = (nhl,nwl,ncl)\n",
    "       \n",
    "    nhl = (nh+2*p-fw)/s + 1\n",
    "    nwl = (nw+2*p-fh)/s + 1\n",
    "    ncl = nc\n",
    "    where\n",
    "       fw,fh : filter sizes\n",
    "       p : padding\n",
    "       s : stride     \n",
    "    '''\n",
    "    nh,nw,nc = inputs\n",
    "    fh,fw = kernel\n",
    "     \n",
    "    s        = stride\n",
    "    p        = padding\n",
    "    ncl      = nc\n",
    "\n",
    "    nhl = (nh+2*p-fw)/s + 1\n",
    "    nwl = (nw+2*p-fh)/s + 1\n",
    "    activation_shape = (int(nhl),int(nwl),int(ncl))\n",
    "\n",
    "\n",
    "    # activation size\n",
    "    activation_size=int(nhl)*int(nwl)*int(ncl)\n",
    "    \n",
    "    \n",
    "    # number Parameters\n",
    "    nparameters=0\n",
    "    \n",
    "    print(\"Activation Shape,\", \"Activation Size,\",\"# Parameters\")\n",
    "     \n",
    "    return   activation_shape ,  activation_size, nparameters    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d9583d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3, 3, 5), 45, 0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = 5,5,5 #nw x nh x nc image\n",
    "kernel  = 3,3      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "\n",
    "dim_pool(inputs, kernel,stride,padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "724356c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3, 3, 1000), 9000, 0)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = 7,7,1000 #nw x nh x nc image\n",
    "kernel  = 2,2      #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "\n",
    "\n",
    "dim_pool(inputs, kernel,stride,padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7dc8c",
   "metadata": {},
   "source": [
    "![convnet](img/lenet5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b75b4",
   "metadata": {},
   "source": [
    "The input layer’s shape is (32, 32, 3), the activation size of that layer is 32 * 32 * 3 = 3072."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b6b8b",
   "metadata": {},
   "source": [
    "### CONV 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "33550862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28, 28, 8)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  =32,32,3  #nw x nh x nc image\n",
    "kernel  = 5,5      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "filters = 8 #6       #number of filters ncl\n",
    "\n",
    "newinput=dim_rgb_convolution(inputs, kernel,stride,padding,filters)\n",
    "newinput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4f550",
   "metadata": {},
   "source": [
    "The activation size for CONV1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ee432f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6272"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28* 28* 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7285635",
   "metadata": {},
   "source": [
    "Parameters  CONV1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc7640",
   "metadata": {},
   "source": [
    "```\n",
    "((fw x fw *nc +1)*ncl)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1de97c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((5*5*3)+1)*8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4dc3fe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((28, 28, 8), 6272, 608)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  =32,32,3  #nw x nh x nc image\n",
    "kernel  = 5,5      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "filters = 8       #number of filters ncl\n",
    "\n",
    "nparameters_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba01989f",
   "metadata": {},
   "source": [
    "### POOL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a3c203e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((14, 14, 8), 1568, 0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = 28, 28, 8 #nw x nh x nc \n",
    "kernel  = 2,2     #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "\n",
    "dim_pool(inputs, kernel,stride,padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b7113",
   "metadata": {},
   "source": [
    "The activation size for POOL1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8207508f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14* 14* 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f1620",
   "metadata": {},
   "source": [
    "### CONV 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5c39e106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 16)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  =14, 14, 8 #nw x nh x nc image\n",
    "kernel  = 5,5      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "filters = 16       #number of filters ncl\n",
    "\n",
    "newinput=dim_rgb_convolution(inputs, kernel,stride,padding,filters)\n",
    "newinput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c4c34",
   "metadata": {},
   "source": [
    "The activation size for CONV2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "321a2c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10*10*16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1af0720e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10, 10, 16), 1600, 3216)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  =14, 14, 8 #nw x nh x nc image\n",
    "kernel  = 5,5      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "filters = 16       #number of filters ncl\n",
    "\n",
    "\n",
    "nparameters_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3274b8",
   "metadata": {},
   "source": [
    "### POOL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bd6ddc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5, 5, 16), 400, 0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = 10, 10, 16 #nw x nh x nc \n",
    "kernel  = 2,2     #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "\n",
    "dim_pool(inputs, kernel,stride,padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566cc0f9",
   "metadata": {},
   "source": [
    "The activation size for POOL2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "edc8a785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5* 5* 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce53d2ff",
   "metadata": {},
   "source": [
    "Parameters in general are weights that are learnt during training. They are weight matrices that contribute to model’s predictive power, changed during back-propagation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4bf56",
   "metadata": {},
   "source": [
    "###  FULLY CONNECTED LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b7d58",
   "metadata": {},
   "source": [
    "To calculate the learnable parameters here, all we have to do is just multiply the by the shape of width hw, height hw, previous layer's filters nc and account for all such filters k in the current layer. Don't forget the bias term for each of the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "937934ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nparameters_fully_connected(c , p):\n",
    "    '''\n",
    "    current layer dimension: c\n",
    "    previous layer activation size: p  \n",
    "    '''\n",
    "    \n",
    "    #activation shape \n",
    "    activation_shape = (c,1)\n",
    "    \n",
    "    # activation size\n",
    "    activation_size=c\n",
    "    \n",
    "    \n",
    "    number=(( c *  p)+1 * c) \n",
    "    print(\"Activation Shape,\", \"Activation Size,\",\"# Parameters\")\n",
    "\n",
    "    return activation_shape, activation_size, number    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09d92b",
   "metadata": {},
   "source": [
    "## FC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "337c0716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((120, 1), 120, 48120)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nparameters_fully_connected(120 , 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab220388",
   "metadata": {},
   "source": [
    "## FC4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61740b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((84, 1), 84, 10164)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nparameters_fully_connected(84 , 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ebcb7",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c6ade34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10, 1), 10, 850)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nparameters_fully_connected(10 , 84)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a76f0eb",
   "metadata": {},
   "source": [
    "Up to now we have seen the dimensions of the activation shape, the activation size and the number of parameters. Let us put in practice this knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def0002",
   "metadata": {},
   "source": [
    "# How to use  AlexNet Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52331199",
   "metadata": {},
   "source": [
    "\n",
    "For this project I will take two differnet models of  AlexNet applied to an unknown dataset from the problem given at the [MMORPG-AI](https://github.com/ruslanmv/BOT-MMORPG-AI)\n",
    "\n",
    "The models to analize are:\n",
    "\n",
    "- Non adapted model\n",
    "- Adapted model\n",
    "\n",
    "The **non adapted model** is just take the \"raw\" definition of the AlexNet Network from the standard python code here\n",
    "\n",
    "\n",
    "The **adapted model** is the version where we modify the parameters of the non adapted model in according to the Analysis previous done in this blog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46936982",
   "metadata": {},
   "source": [
    "Let us first load the libraries that we need to begin the discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a6fcd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Gamepad library\n",
    "from mmorpg import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b9dfa",
   "metadata": {},
   "source": [
    "The important part is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c4d02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the size of the pictures\n",
    "WIDTH = 480\n",
    "HEIGHT = 270"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7769c1c",
   "metadata": {},
   "source": [
    "We load the data of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "63b71a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load the images of the gameplay\n",
    "x_training_data=pd.read_pickle('data/dfx-0.pkl')  \n",
    "#We load the inputs of the of the gameplay\n",
    "y_training_data=pd.read_pickle('data/dfy-0.pkl')  \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_training_data, y_training_data, test_size=0.2, random_state=6)\n",
    "# Train Image part ( 4 Dimensional)\n",
    "X_image = np.array([df_to_numpy_image(X_train,i) for i in X_train.index])\n",
    "X=X_image.reshape(-1,WIDTH,HEIGHT,3)\n",
    "#Train Input part ( 1 Dimensional )\n",
    "Y = [df_to_numpy_input(y_train,i) for i in y_train.index]\n",
    "# Test Image part ( 4 Dimensional)\n",
    "test_image = np.array([df_to_numpy_image(X_valid,i) for i in X_valid.index])\n",
    "test_x=test_image.reshape(-1,WIDTH,HEIGHT,3)\n",
    "## Test Input part( 1 Dimensional )\n",
    "test_y = [df_to_numpy_input(y_valid,i) for i in y_valid.index]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68faf81",
   "metadata": {},
   "source": [
    "# Alexnet Model - Non adapted model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725eb99e",
   "metadata": {},
   "source": [
    "We define the standard AlexNet non adapted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe3526dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "MODEL_NAME = 'mmorpg-{}-{}.model'.format(LR, 'alexnet-non-adapted') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b18a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(width, height, lr, output=29):\n",
    "    # Building 'AlexNet'                                                  #line\n",
    "    network = input_data(shape=[None, width, height, 3])                  #0\n",
    "    network = conv_2d(network, 96, 11, strides=4, activation='relu')      #1\n",
    "    network = max_pool_2d(network, 3, strides=2)                          #2\n",
    "    network = local_response_normalization(network)                       #3\n",
    "    network = conv_2d(network, 256, 5, activation='relu')                 #4\n",
    "    network = max_pool_2d(network, 3, strides=2)                          #5\n",
    "    network = local_response_normalization(network)                       #6\n",
    "    network = conv_2d(network, 384, 3, activation='relu')                 #7\n",
    "    network = conv_2d(network, 384, 3, activation='relu')                 #8\n",
    "    network = conv_2d(network, 256, 3, activation='relu')                 #9\n",
    "    network = max_pool_2d(network, 3, strides=2)                          #10\n",
    "    network = local_response_normalization(network)                       #11\n",
    "    network = fully_connected(network, 4096, activation='tanh')           #12\n",
    "    network = dropout(network, 0.5)                                       #13\n",
    "    network = fully_connected(network, 4096, activation='tanh')           #14\n",
    "    network = dropout(network, 0.5)                                       #15\n",
    "    network = fully_connected(network, 29, activation='softmax')          #16\n",
    "    network = regression(network, optimizer='momentum',                   #17\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, checkpoint_path='model_alexnet',\n",
    "                        max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir='log')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "470a8fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alexnet(WIDTH, HEIGHT, LR, output=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf31308",
   "metadata": {},
   "source": [
    "We train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50de177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.97406\u001b[0m\u001b[0m | time: 21.022s\n",
      "\u001b[2K\r",
      "| Momentum | epoch: 005 | loss: 1.97406 - acc: 0.4897 -- iter: 180/180\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, n_epoch=5, validation_set=0.1, shuffle=True,\n",
    "              show_metric=True, batch_size=64, snapshot_step=200,\n",
    "              snapshot_epoch=False, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb5fd7",
   "metadata": {},
   "source": [
    "We have seen that the accuracy is less than **0.5** and the loss near to **2.0** . With the knowledge of the dimensions studied before we will adapt the model in appropiate way **to improve** the AlexNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c53b1",
   "metadata": {},
   "source": [
    "### Understanding the parameters of AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c753b2ed",
   "metadata": {},
   "source": [
    "The standard AlexNet network may be depicted as the Cousera Deep Learning Course:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f21199",
   "metadata": {},
   "source": [
    "![AlexNet](img/alex.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e166c7d8",
   "metadata": {},
   "source": [
    "Where we obtain the essential parameters for each of the layers depicted in the previous picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a74cb1",
   "metadata": {},
   "source": [
    "The inputs of the neural nework in tensorflow is given by\n",
    "```\n",
    "input_data(shape=[None, width, height, 3])                  #0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "65b6074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55, 55, 96), 290400, 34944)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONV 1\n",
    "inputs  =227,227,3  #nw x nh x nc image\n",
    "kernel  = 11,11      #fw x fw  filter\n",
    "stride  = 4.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "filters = 96       #number of filters ncl\n",
    "nparameters_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433cf7f",
   "metadata": {},
   "source": [
    "In TensorFlow this part corresponds to \n",
    "```\n",
    "conv_2d(network, 96, 11, strides=4, activation='relu')      #1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "fbad6207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((27, 27, 96), 69984, 0)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POOL1\n",
    "inputs  = 55, 55, 96 #nw x nh x nc \n",
    "kernel  = 3,3     #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "dim_pool(inputs, kernel,stride,padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d27b7ce",
   "metadata": {},
   "source": [
    "In TensorFlow this part corresponds to \n",
    "```\n",
    "max_pool_2d(network, 3, strides=2)                          #2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73451236",
   "metadata": {},
   "source": [
    "After using a pool we can use a normalization\n",
    "```\n",
    "local_response_normalization(network)                       #3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "9b1e3d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((27, 27, 256), 186624, 614656)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONVOLUTION SAME 1\n",
    "inputs  =27, 27, 96 #nw x nh x nc image\n",
    "kernel  = 5,5      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 2.0      #padding p\n",
    "filters = 256       #number of filters ncl\n",
    "nparameters_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a2970",
   "metadata": {},
   "source": [
    "In TensorFlow this part corresponds to:\n",
    "```\n",
    "conv_2d(network, 256, 5, activation='relu')                 #4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "25dfef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((13, 13, 256), 43264, 0)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POOL2\n",
    "inputs  = 27, 27, 256 #nw x nh x nc \n",
    "kernel  = 3,3     #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "dim_pool(inputs, kernel,stride,padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722b036",
   "metadata": {},
   "source": [
    "In TensorFlow this part corresponds to:\n",
    "```\n",
    "max_pool_2d(network, 3, strides=2)                          #5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34fec7",
   "metadata": {},
   "source": [
    "After a pool in we use:\n",
    "\n",
    "```\n",
    "local_response_normalization(network)                       #6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "b2a48653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((13, 13, 384), 64896, 885120)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONVOLUTION SAME 2\n",
    "inputs  =13, 13, 256 #nw x nh x nc image\n",
    "kernel  = 3,3      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 1.0      #padding p\n",
    "filters = 384       #number of filters ncl\n",
    "nparameters_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c19eba",
   "metadata": {},
   "source": [
    "In TensorFlow this part corresponds to:\n",
    "```\n",
    "conv_2d(network, 384, 3, activation='relu')                 #7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9af995cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((13, 13, 384), 64896, 1327488)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONVOLUTION SAME 3\n",
    "inputs  =13, 13, 384 #nw x nh x nc image\n",
    "kernel  = 3,3      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 1.0      #padding p\n",
    "filters = 384       #number of filters ncl\n",
    "nparameters_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba2a60",
   "metadata": {},
   "source": [
    "In TensorFlow this part corresponds to:\n",
    "```\n",
    "conv_2d(network, 384, 3, activation='relu')                 #8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "5cac80cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((13, 13, 256), 43264, 884992)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONVOLUTION SAME 4\n",
    "inputs  =13, 13, 384 #nw x nh x nc image\n",
    "kernel  = 3,3      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 1.0      #padding p\n",
    "filters = 256       #number of filters ncl\n",
    "nparameters_convolution(inputs, kernel,stride,padding,filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6376835a",
   "metadata": {},
   "source": [
    "In TensorFlow this part corresponds to:\n",
    "```\n",
    "conv_2d(network, 256, 3, activation='relu')                 #9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "fcc35704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((6, 6, 256), 9216, 0)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POOL3\n",
    "inputs  = 13, 13, 256 #nw x nh x nc \n",
    "kernel  = 3,3     #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "dim_pool(inputs, kernel,stride,padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6770cc3",
   "metadata": {},
   "source": [
    "In TensorFlow this part corresponds to:\n",
    "```\n",
    "max_pool_2d(network, 3, strides=2)                          #10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ab075",
   "metadata": {},
   "source": [
    "After pool we use a normalization\n",
    "```\n",
    "local_response_normalization(network)                       #11\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "90b235a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4096, 1), 4096, 37752832)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FC1\n",
    "nparameters_fully_connected(4096 , 9216)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d7626",
   "metadata": {},
   "source": [
    "In TensorFlow this part corresponds to:\n",
    "```\n",
    "fully_connected(network, 4096, activation='tanh')           #12\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b9f03",
   "metadata": {},
   "source": [
    "Dropout can be used after convolutional layers (e.g. Conv2D) and after pooling layers (e.g. MaxPooling2D). Often, dropout is only used after the pooling layers, but this is just a rough heuristic. After fully connected layer we use  dropout to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e622a94",
   "metadata": {},
   "source": [
    "```\n",
    "dropout(network, 0.5)                                       #13\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "bc995201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4096, 1), 4096, 16781312)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FC2\n",
    "nparameters_fully_connected(4096 , 4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa548ff9",
   "metadata": {},
   "source": [
    "In TensorFlow this part corresponds to:\n",
    "```\n",
    "fully_connected(network, 4096, activation='tanh')           #14\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5d9a2",
   "metadata": {},
   "source": [
    "After fully connected layer we use dropout \n",
    "```\n",
    "dropout(network, 0.5)                                       #15\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "7042807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1000, 1), 1000, 4097000)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Softmax\n",
    "nparameters_fully_connected(1000 , 4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6a8b5",
   "metadata": {},
   "source": [
    "In TensorFlow this part corresponds to:\n",
    "```\n",
    "fully_connected(network, 29, activation='softmax')          #16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb458ea",
   "metadata": {},
   "source": [
    "## Full code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052e917",
   "metadata": {},
   "source": [
    "Let us write the parameters Alexnet network in asimple code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9eaa8c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    }
   ],
   "source": [
    "parameters={}\n",
    "\n",
    "#Input layer\n",
    "parameters[0]=227,227,3\n",
    "\n",
    "#CONV 1\n",
    "inputs  =parameters[0]  #nw x nh x nc image\n",
    "kernel  = 11,11      #fw x fw  filter\n",
    "stride  = 4.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "filters = 96       #number of filters ncl\n",
    "parameters[1]=nparameters_convolution(inputs, kernel,stride,padding,filters)\n",
    "#POOL1\n",
    "inputs  = parameters[1][0] #nw x nh x nc \n",
    "kernel  = 3,3     #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "parameters[2]=dim_pool(inputs, kernel,stride,padding)\n",
    "#CONVOLUTION SAME 1\n",
    "inputs  =parameters[2][0] #nw x nh x nc image\n",
    "kernel  = 5,5      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 2.0      #padding p\n",
    "filters = 256       #number of filters ncl\n",
    "parameters[3]=nparameters_convolution(inputs, kernel,stride,padding,filters)\n",
    "check_same(inputs,parameters[3]) # Checking parameters of same convolution\n",
    "\n",
    "#POOL2\n",
    "inputs  = parameters[3][0] #nw x nh x nc \n",
    "kernel  = 3,3     #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "parameters[4]=dim_pool(inputs, kernel,stride,padding)\n",
    "\n",
    "#CONVOLUTION SAME 2\n",
    "inputs  =parameters[4][0] #nw x nh x nc image\n",
    "kernel  = 3,3      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 1.0      #padding p\n",
    "filters = 384       #number of filters ncl\n",
    "parameters[5]=nparameters_convolution(inputs, kernel,stride,padding,filters)\n",
    "check_same(inputs,parameters[5]) # Checking parameters of same convolution\n",
    "\n",
    "\n",
    "#CONVOLUTION SAME 3\n",
    "inputs  =parameters[5][0] #nw x nh x nc image\n",
    "kernel  = 3,3      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 1.0      #padding p\n",
    "filters = 384       #number of filters ncl\n",
    "parameters[6]=nparameters_convolution(inputs, kernel,stride,padding,filters)\n",
    "check_same(inputs,parameters[6]) # Checking parameters of same convolution\n",
    "\n",
    "\n",
    "#CONVOLUTION SAME 4\n",
    "inputs  =parameters[6][0] #nw x nh x nc image\n",
    "kernel  = 3,3      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 1.0      #padding p\n",
    "filters = 256       #number of filters ncl\n",
    "parameters[7]=nparameters_convolution(inputs, kernel,stride,padding,filters)\n",
    "check_same(inputs,parameters[7]) # Checking parameters of same convolution\n",
    "\n",
    "#POOL3\n",
    "inputs  = parameters[7][0] #nw x nh x nc \n",
    "kernel  = 3,3     #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "parameters[8]=dim_pool(inputs, kernel,stride,padding)\n",
    "#FC1\n",
    "parameters[9]=nparameters_fully_connected(4096 , parameters[8][1])\n",
    "#FC2\n",
    "parameters[10]=nparameters_fully_connected(parameters[9][1] , parameters[9][1])\n",
    "#Softmax\n",
    "parameters[11]=nparameters_fully_connected(1000 , parameters[10][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "d7fa9249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation, Activation Shape, Activation Size, #Parameters\n",
      "0 (227, 227, 3)\n",
      "1 ((55, 55, 96), 290400, 34944)\n",
      "2 ((27, 27, 96), 69984, 0)\n",
      "3 ((27, 27, 256), 186624, 614656)\n",
      "4 ((13, 13, 256), 43264, 0)\n",
      "5 ((13, 13, 384), 64896, 885120)\n",
      "6 ((13, 13, 384), 64896, 1327488)\n",
      "7 ((13, 13, 256), 43264, 884992)\n",
      "8 ((6, 6, 256), 9216, 0)\n",
      "9 ((4096, 1), 4096, 37752832)\n",
      "10 ((4096, 1), 4096, 16781312)\n",
      "11 ((1000, 1), 1000, 4097000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Operation,\",\"Activation Shape,\", \"Activation Size,\",\"#Parameters\")\n",
    "for i in range(12):\n",
    "    step=i\n",
    "    layer=parameters[step]\n",
    "    print(step, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a280a",
   "metadata": {},
   "source": [
    "From the standard framework of the **AlexNet** we see that:\n",
    "\n",
    "The  **original** input pictures have the dimensions of \n",
    "\n",
    "**227x227x3**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29523f33",
   "metadata": {},
   "source": [
    "and our pictutes in the **MMORPG-AI** project are:\n",
    "\n",
    "**270x 480x3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e963a7",
   "metadata": {},
   "source": [
    "That means that we have to adapt the template AlexNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9c6eeafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(270, 480, 3)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showarray(X_image[0])\n",
    "X_image[0].shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ddbda2",
   "metadata": {},
   "source": [
    "We should **modify** al the whole neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f668bb",
   "metadata": {},
   "source": [
    "# Modified version of AlexNet - Adapted version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd560c",
   "metadata": {},
   "source": [
    "Let us write the parameters Alexnet network in asimple code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "b3e44173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n",
      "Activation Shape, Activation Size, # Parameters\n"
     ]
    }
   ],
   "source": [
    "parameters={}\n",
    "\n",
    "#Input layer\n",
    "parameters[0]=270,480,3\n",
    "\n",
    "#CONV 1\n",
    "inputs  =parameters[0]  #nw x nh x nc image\n",
    "kernel  = 11,11      #fw x fw  filter\n",
    "stride  = 4.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "filters = 96       #number of filters ncl\n",
    "parameters[1]=nparameters_convolution(inputs, kernel,stride,padding,filters)\n",
    "#POOL1\n",
    "inputs  = parameters[1][0] #nw x nh x nc \n",
    "kernel  = 3,3     #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "parameters[2]=dim_pool(inputs, kernel,stride,padding)\n",
    "#CONVOLUTION SAME 1\n",
    "inputs  =parameters[2][0] #nw x nh x nc image\n",
    "kernel  = 5,5      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 2.0      #padding p\n",
    "filters = 256       #number of filters ncl\n",
    "parameters[3]=nparameters_convolution(inputs, kernel,stride,padding,filters)\n",
    "check_same(inputs,parameters[3]) # Checking parameters of same convolution\n",
    "\n",
    "#POOL2\n",
    "inputs  = parameters[3][0] #nw x nh x nc \n",
    "kernel  = 3,3     #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "parameters[4]=dim_pool(inputs, kernel,stride,padding)\n",
    "\n",
    "#CONVOLUTION SAME 2\n",
    "inputs  =parameters[4][0] #nw x nh x nc image\n",
    "kernel  = 3,3      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 1.0      #padding p\n",
    "filters = 384       #number of filters ncl\n",
    "parameters[5]=nparameters_convolution(inputs, kernel,stride,padding,filters)\n",
    "check_same(inputs,parameters[5]) # Checking parameters of same convolution\n",
    "\n",
    "\n",
    "#CONVOLUTION SAME 3\n",
    "inputs  =parameters[5][0] #nw x nh x nc image\n",
    "kernel  = 3,3      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 1.0      #padding p\n",
    "filters = 384       #number of filters ncl\n",
    "parameters[6]=nparameters_convolution(inputs, kernel,stride,padding,filters)\n",
    "check_same(inputs,parameters[6]) # Checking parameters of same convolution\n",
    "\n",
    "\n",
    "#CONVOLUTION SAME 4\n",
    "inputs  =parameters[6][0] #nw x nh x nc image\n",
    "kernel  = 3,3      #fw x fw  filter\n",
    "stride  = 1.0      #stride s\n",
    "padding = 1.0      #padding p\n",
    "filters = 256       #number of filters ncl\n",
    "parameters[7]=nparameters_convolution(inputs, kernel,stride,padding,filters)\n",
    "check_same(inputs,parameters[7]) # Checking parameters of same convolution\n",
    "\n",
    "#POOL3\n",
    "inputs  = parameters[7][0] #nw x nh x nc \n",
    "kernel  = 3,3     #fw x fw  filter\n",
    "stride  = 2.0      #stride s\n",
    "padding = 0.0      #padding p\n",
    "parameters[8]=dim_pool(inputs, kernel,stride,padding)\n",
    "#FC1\n",
    "parameters[9]=nparameters_fully_connected(4096 , parameters[8][1])\n",
    "#FC2\n",
    "parameters[10]=nparameters_fully_connected(parameters[9][1] , parameters[9][1])\n",
    "#Softmax\n",
    "parameters[11]=nparameters_fully_connected(1000 , parameters[10][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "68160ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation, Activation Shape, Activation Size, #Parameters\n",
      "0 (270, 480, 3)\n",
      "1 ((65, 118, 96), 736320, 34944)\n",
      "2 ((32, 58, 96), 178176, 0)\n",
      "3 ((32, 58, 256), 475136, 614656)\n",
      "4 ((15, 28, 256), 107520, 0)\n",
      "5 ((15, 28, 384), 161280, 885120)\n",
      "6 ((15, 28, 384), 161280, 1327488)\n",
      "7 ((15, 28, 256), 107520, 884992)\n",
      "8 ((7, 13, 256), 23296, 0)\n",
      "9 ((4096, 1), 4096, 95424512)\n",
      "10 ((4096, 1), 4096, 16781312)\n",
      "11 ((1000, 1), 1000, 4097000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Operation,\",\"Activation Shape,\", \"Activation Size,\",\"#Parameters\")\n",
    "for i in range(12):\n",
    "    step=i\n",
    "    layer=parameters[step]\n",
    "    print(step, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c752d49",
   "metadata": {},
   "source": [
    "The previous results shows how we should modify all the layers in an appropiate way if we want to follow the same structure of the AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e59db7",
   "metadata": {},
   "source": [
    "First let us define all the parameters of AlexNet defined before.\n",
    "We will consider 14 lines of parameters, defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58aa2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter1a=96 \n",
    "parameter1b=11\n",
    "parameter1c=4\n",
    "\n",
    "parameter2a=3\n",
    "parameter2b=2\n",
    "\n",
    "\n",
    "parameter3b=3\n",
    "parameter3a=256\n",
    "\n",
    "\n",
    "parameter4a=3\n",
    "parameter4b=2\n",
    "\n",
    "\n",
    "parameter5b=3\n",
    "parameter5a=384\n",
    "\n",
    "parameter6b=3\n",
    "parameter6a=384\n",
    "\n",
    "parameter7b=3\n",
    "parameter7a=256\n",
    "\n",
    "\n",
    "paramter8a=3\n",
    "paramter8b=2\n",
    "\n",
    "parameter9b=4096\n",
    "\n",
    "paramter10a=0.5\n",
    "parameter11a=4096\n",
    "\n",
    "paramter12a=0.5\n",
    "parameter13a=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d643450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet_adapted(width, height, lr, output=29):\n",
    "    # Building 'AlexNet'\n",
    "    network = input_data(shape=[None, width, height, 3])                                         #0\n",
    "    network = conv_2d(network, parameter1a, parameter1b, strides=parameter1c, activation='relu') #1\n",
    "    network = max_pool_2d(network, parameter2a, strides=parameter2b)                             #2\n",
    "    network = local_response_normalization(network)                  \n",
    "    network = conv_2d(network, parameter3a, parameter3b, activation='relu')                      #3\n",
    "    network = max_pool_2d(network, parameter4a, strides=parameter4b)                             #4\n",
    "    network = local_response_normalization(network)                  \n",
    "    network = conv_2d(network, parameter5a, parameter5b, activation='relu')                      #5\n",
    "    network = conv_2d(network, parameter6a, parameter6b, activation='relu')                      #6\n",
    "    network = conv_2d(network, parameter7a, parameter7b, activation='relu')                      #7\n",
    "    network = max_pool_2d(network, paramter8a, strides=paramter8b)                               #8\n",
    "    network = local_response_normalization(network)                  \n",
    "    network = fully_connected(network, parameter9b, activation='tanh')                           #9\n",
    "    network = dropout(network, paramter10a)                                                      #10\n",
    "    network = fully_connected(network, parameter11a, activation='tanh')                          #11\n",
    "    network = dropout(network, paramter12a)                                                      #12\n",
    "    network = fully_connected(network, 29, activation='softmax')     \n",
    "    network = regression(network, optimizer='momentum',                                          #13\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=parameter13a)  \n",
    "\n",
    "    # Training\n",
    "    model = tflearn.DNN(network, checkpoint_path='model_alexnet_adapted',\n",
    "                        max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir='log')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99c70b",
   "metadata": {},
   "source": [
    "**Learning rate** \n",
    "The learning rate defines how quickly a network updates its parameters.\n",
    "\n",
    "Low learning rate slows down the learning process but converges smoothly. Larger learning rate speeds up the learning but may not converge.\n",
    "\n",
    "Usually a decaying Learning rate is preferred.\n",
    "\n",
    "**Momentum**\n",
    "Momentum helps to know the direction of the next step with the knowledge of the previous steps. It helps to prevent oscillations. A typical choice of momentum is between 0.5 to 0.9.\n",
    "\n",
    "**Number of epochs**\n",
    "Number of epochs is the number of times the whole training data is shown to the network while training.\n",
    "\n",
    "Increase the number of epochs until the validation accuracy starts decreasing even when training accuracy is increasing(overfitting).\n",
    "\n",
    "**Batch size**\n",
    "Mini batch size is the number of sub samples given to the network after which parameter update happens.\n",
    "\n",
    "A good default for batch size might be 32. Also try 32, 64, 128, 256, and so o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23570d",
   "metadata": {},
   "source": [
    "The adapted version of the AlexNet model does not modify the latest size of the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e066ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mmorpg-0.001-alex-adapted.model'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "MODEL_NAME = 'mmorpg-{}-{}.model'.format(LR, 'alex-adapted') \n",
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f756e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alexnet_adapted(WIDTH, HEIGHT, LR, output=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39569c",
   "metadata": {},
   "source": [
    "We train the modifed  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcf7fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.88725\u001b[0m\u001b[0m | time: 20.748s\n",
      "\u001b[2K\r",
      "| Momentum | epoch: 006 | loss: 1.88725 - acc: 0.4195 -- iter: 180/180\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, n_epoch=5, validation_set=0.1, shuffle=True,\n",
    "              show_metric=True, batch_size=64, snapshot_step=200,\n",
    "              snapshot_epoch=False, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b3e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pygta5)",
   "language": "python",
   "name": "pygta5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
